<!DOCTYPE html>
<!-- saved from url=(0055)http://strata.uga.edu/6370/lecturenotes/regression.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<meta charset="utf-8">
	<title>Data Analysis in the Geosciences</title>
	<meta name="generator" content="BBEdit 9.6">
	<link rel="stylesheet" href="./Model 2 regression_files/dataAnalysis.css" type="text/css" media="screen">	
	<link rel="stylesheet" href="./Model 2 regression_files/dataAnalysisPrint.css" type="text/css" media="print">
</head>
<body>

<div id="Header">
	<h1><a href="http://strata.uga.edu/6370/index.html">Data Analysis in the Geosciences</a></h1>
	<h2><a href="http://strata.uga.edu/6370/index.html">GEOL 6370</a></h2>
</div>

<div id="MainContent">
	<div id="ListColumn">
		<h1><a href="http://strata.uga.edu/6370/lecturenotes/index.html">Lecture Notes</a></h1>
		<h1><a href="http://strata.uga.edu/6370/index.html">Home</a></h1>
		<h1>Contact</h1>
		<p><a href="mailto:stratum@uga.edu?subject=6370">Steven Holland</a></p>
	</div>
	
	<div id="ContentColumn">

<h1>Regression</h1>

<p>You often want to know the mathematical form of a relationship between two or more variables and regression is the statistical method for finding that relationship. You might use the equation you get from regression solely for <i>description</i> or possibly for <i>prediction</i>, that is, predicting a dependent (y) variable when you have measured only the independent (x) variable. You might also want to <i>test hypotheses</i> about the slope and y-intercept. Although correlation and regression are commonly confused, regression describes the mathematical relationship and correlation describes its strength. There are several ways to fit a line to data and this lecture will cover some of the most common.</p>


<h2>Model 1: Least-squares regression</h2>

<p>Least-squares regression is the most common method for fitting a line, and for many statistical packages, it is the only way. The linear mathematical model for the relationship between two variables, an independent X variable and a dependent Y variable is described by the following equation</p>

<img src="./Model 2 regression_files/leastSquaresLinearModel.png" alt="least squares linear model">

<p>where slope is β<sub>1</sub> and the y-intercept is β<sub>0</sub>. The value of any observed Y<sub>i</sub> is equal to the slope multiplied by the value of X<sub>i</sub> plus the y-intercept, plus an error term ε<sub>i</sub>. The error term represents other factors besides X that influence the value of Y, including measurement errors and other unmeasured variables. Error in X is assumed to be minor, meaning that you set X and measure the resulting Y; that is, that X is under your direct control, but Y is not. Errors in Y are assumed to be normally distributed about the regression line.</p>

<p>The method of least-squares fits a line that minimizes the sums of squares of the residuals, that is, the difference between the observed Y<sub>i</sub> and predicted Ŷ<sub>i</sub>, hence the name least-squares regression. The least-squares regression can be used for description, prediction, and hypothesis testing.</p>

<p>The <b>slope</b> of the line is calculated as:</p>

<img src="./Model 2 regression_files/leastSquaresSlope.png" alt="least-squares slope">

<p>Depending on your circumstances, you may wish to calculate it as the sum of products divided by the sum of squares in x, by the covariance divided by the variance in x, or as the correlation coefficient multiplied by the ratio of standard deviations. All are equivalent.</p>

<p>The <b>Y-intercept</b> is calculated as:</p>

<img src="./Model 2 regression_files/leastSquaresIntercept.png" alt="least-squares intercept">

<p>where Y-bar and X-bar are the mean of Y and X, known as the <b>centroid</b>. The regression line is constrained pass through the centroid of the data.</p>

<p>Everything to this point is descriptive, in that the statistics for slope and intercept are calculated, but no inferences are made about the population. If you wish to make statistical inferences about the parameters (the slope and intercept of the population), there are several questions you could ask, such as:</p>

<ul>
	<li>Is the slope significantly different from zero?</li>
	<li>Is the y-intercept significantly different from zero?</li>
	<li>How much variance in Y does the regression explain?</li>
</ul>

<p>In evaluating the regression, we would also want to ask:</p>

<ul>
	<li>Is the relationship between X and Y a linear one, or does other function better describe it?</li>
	<li>Are the residuals normally distributed?</li>
	<li>Are the residuals independent of X, or do they change systematically with X?</li>
</ul>


<h3>Regression in R</h3>

<p>You can perform a regression in R with the <span class="codeSpan">lm()</span> function (think <b>l</b>inear <b>m</b>odel). The <span class="codeSpan">lm()</span> function is exceptionally powerful and for now I will demonstrate only a simple linear regression of two variables. First, I will simulate some data for a line of known slope and intercept, so that we can evaluate the results of the regression.</p>

<p class="codeParagraph">
x &lt;- rnorm(n=30, mean=15, sd=3)<br>
errors &lt;- rnorm(n=30, sd=2)<br>
slope &lt;- 2.15<br>
intercept &lt;- 4.64<br>
y &lt;- slope*x + intercept + errors
</p>

<p>Always plot the data first to check that the relationship looks linear and that there are no outliers in either x or y.</p>

<p class="codeParagraph">
plot(x, y, pch=16)
</p>

<p>Run the regression with the <span class="codeSpan">lm()</span> function. In this example, y is modeled as a linear function of x. Assign the results of the regression to an object so you can fully analyze the results.</p>

<p class="codeParagraph">
myRegression &lt;- lm(y~x)
</p>

<p>Read the notation <span class="codeSpan">lm(y~x)</span> as “a linear model, with y as a function of x”. Other relationships can be modeled besides a simple linear one. For example, if you wanted y as a function of x<sup>2</sup>, you could write <span class="codeSpan">lm(y~x^2)</span>, and so on for other relationships.</p>

<p>The <span class="codeSpan">myRegression</span> object contains the statistics for the y-intercept (Intercept) and slope (x), among other things.</p>

<p class="codeParagraph">
&gt; myRegression<br>
&nbsp;<br>
Call:<br>
lm(formula = y ~ x)<br>
&nbsp;<br>
Coefficients:<br>
(Intercept)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;x<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.914&nbsp;&nbsp;&nbsp;&nbsp;2.068
</p>

<p>Use the <span class="codeSpan">names()</span> function to view everything contained in the <span class="codeSpan">myRegression</span> object. You can access any of these by name or by position in the vector. For example, to retrieve the residuals, you could use <span class="codeSpan">myRegression$residuals</span> or <span class="codeSpan">myRegression[2]</span>.</p>

<p class="codeParagraph">
&gt; names(myRegression)<br>
&nbsp;[1]&nbsp;"coefficients"&nbsp;&nbsp;"residuals"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"effects"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"rank"<br>
&nbsp;[5]&nbsp;"fitted.values"&nbsp;"assign"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"qr"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"df.residual"<br>
&nbsp;[9]&nbsp;"xlevels"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"call"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"terms"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"model"
</p>


<h3>Hypothesis tests</h3>

<p>You can perform hypothesis tests on the regression with the <span class="codeSpan">summary()</span> function.</p>

<p class="codeParagraph">
&gt; summary(myRegression)<br>
&nbsp;
Call:<br>
lm(formula = y ~ x)<br>
&nbsp;<br>
Residuals:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Min&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1Q&nbsp;&nbsp;&nbsp;Median&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3Q&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Max<br>
-4.6106&nbsp;&nbsp;-1.0806&nbsp;&nbsp;&nbsp;0.3249&nbsp;&nbsp;&nbsp;1.0416&nbsp;&nbsp;&nbsp;4.2920<br>
&nbsp;<br>
Coefficients:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Estimate&nbsp;&nbsp;&nbsp;Std. Error&nbsp;&nbsp;&nbsp;t value&nbsp;&nbsp;&nbsp;Pr(&gt;|t|)<br>
(Intercept)&nbsp;&nbsp;6.9137&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2027&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.139&nbsp;&nbsp;&nbsp;&nbsp;0.00397 ** <br>
x&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.0676&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.1404&nbsp;&nbsp;&nbsp;&nbsp;14.726&nbsp;&nbsp;&nbsp;1.03e-14 ***<br>
---<br>
Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1<br>
&nbsp;
Residual standard error: 1.867 on 28 degrees of freedom<br>
Multiple R-squared: 0.8856,	Adjusted R-squared: 0.8816 <br>
F-statistic: 216.8 on 1 and 28 DF, p-value: 1.034e-14
</p>

<p>There’s a lot of information in the output, so let’s step through this. First, the function call is displayed, so you know what was calculated.</p>

<p>Next shown are the residuals, and here you can evaluate the assumption of whether the <b>residuals are normally distributed</b>. Since the errors are assumed to be centered about the regression line, the median residual should be close to zero. The errors are assumed to be normally distributed, so the first quartile (1Q) and third quartile (3Q) residuals should be about equally distant from zero. Likewise, the minimum (Min) and maximum (Max) residual should be roughly equally distant from zero. In this case, everything looks roughly balanced, although the median is 0.3, which might be a warning sign than the residuals may be skewed.</p>

<p>The coefficients of the regression line are shown next, along with tests of the null hypotheses (i.e., slope equals zero and intercept equals zero). The estimates of the intercept and slope are shown in the second column (here, 6.9137 and 2.0676). The labeling of slope as x may seem confusing at first, but in a multiple regression, where you regress one dependent variable (y) against multiple independent variables, such as Iron, Chloride, and Phosphorous, the slope for each independent variable will appear on its own line, labeled with their name.</p>

<p>Both the intercept and the slope follow a t-distribution, if there is random sampling and the errors are normally distributed. To run a <b>t-test against null hypotheses of zero intercept and zero slope</b>, you need the standard errors of the intercept and slope (third column) to calculate a t-statistic (fourth column). The corresponding p-values are shown in the last column. For convenience, these are labeled with asterisks to indicate their statistical significance, with *** indicating significance at the 0.001 level, ** indicating significance at the 0.01 level, * indicating significance at the 0.05 level, . indicating significance at the 0.1 level, and no symbol indicating significance at greater than 0.1.</p>

<p>The residual standard error is shown on the next line. The standard error of the residuals is calculated as the square root of the sum of squares of the residuals, divided by the degrees of freedom (n-2). The residual standard error gives you a measure of the amount of scatter around the line. You can compare this value to the standard deviation in y to get a sense for the amount of variation in y that is not accounted for by the regression.</p>

<p>The R-squared value indicates the <b>proportion of explained variation</b>, that is, the variation in y that is explained by the variation in x. It is calculated as the square of the correlation coefficient. If all of the y-values fell exactly on the regression line, x would be a perfect predictor of y and it would be said to explain all of the variation in y. If the points formed a shotgun blast where the correlation coefficient was near zero, the percent of explained variation (R<sup>2</sup>) would be very close to zero, indicating that the regression explains almost nothing about the variation in y. R-squared is an important statistic and should always be reported; it is the best single-number of summary of the strength of a linear relationship. The adjusted R-squared reflects a small penalty correction for the number of included parameters (independent variables). As you add more independent variables, the adjusted R-squared value will become progressively smaller than the R-squared value.</p>

<p>The <b>test of the significance of the regression</b> itself is shown on the last line as an F-test, a ratio of variances. This tests whether the regression variance accounted for by the regression is greater than variance in the residuals. If the regression explains none of the variation in Y, these two variances should be equal. The low p-value here tells you that the regression is highly significant: it explains much of the variance in y. The degrees of freedom are 1 for the numerator (the regression) and 28 (n-2) for the denominator. Bear in mind that this F-test will only tell you whether there is a statistically significant result. Like all p-values, this is strongly influenced by sample size and tells you nothing about effect size and our uncertainty in that. To determine <b>effect size</b>, examine the size of the slope and use its standard error to calculate a confidence interval.</p>

<p>It is straightforward to get confidence intervals on the slope and intercept. Because you know the standard errors, you could also do this longhand.</p>

<p class="codeParagraph">
confint(myRegression)
</p>


<h3>Evaluating the assumptions</h3>

<p>Before using our results, you should evaluate several aspects of the regression and you can do this with the <span class="codeSpan">plot()</span> command, which will display four useful plots, one at a time:</p>

<p class="codeParagraph">
plot(myRegression)
</p>

<p>The first image it will display shows residuals (ε) plotted against the fitted values (Ŷ). The residuals should display no pattern: they should not show a trend along the x-axis and their variance should not change along the x-axis. The residuals should be centered around zero (the gray dashed line). The red line is a trend line to help you visualize changes in residuals as a function of the fitted values; it should coincide with the gray dashed line. Any numbered points in this or the other plots are points that should be scrutinized, particularly so if the same points are flagged in multiple plots.</p>

<p>The second image is a <span class="codeSpan">qqnorm()</span> plot. The residuals are assumed to be normally distributed, so they should fall fairly close to the gray dashed line on this plot. Strong systematic departures indicate non-normal errors.</p>

<p>The third image is similar to the first, but it plots the square roots of the residuals, making all of the values positive. The points should not form a strong triangular shape with increasing values as the fitted values increase.</p>

<p>The fourth image plots the standardized residuals against leverage, giving you clues as to which data points have the greatest influence on the estimates of slope and intercept. Note that this is a different plot than indicated in your text (p. 144), but the goal is the same. Numbered points are points that fall far from the line or that are outliers. It is often useful to check these points to make sure that their values were entered correctly and that there is nothing unusual or disconcerting about those measurements.</p>


<h3>Adding the regression to a plot</h3>

<p>While our plot is still active, you can add the regression line to it with the <span class="codeSpan">abline()</span> function. You can stylize the line as you normally would.</p>

<p class="codeParagraph">
plot(x, y, pch=16)<br>
abline(myRegression, col='red', lwd=2)
</p>


<h3>Prediction using the regression</h3>

<p>In some cases, you would like to use the regression for prediction, that is, you know the independent (predictor) variable and you would like to know the value of the dependent (result) variable. To do this, you need to supply the regression model and a list of values of the independent variable for which you would like predictions.</p>

<p class="codeParagraph">
predict(myRegression, list(x=c(12.2, 13.7, 14.45)))
</p>


<h2>Model 2 regression</h2>

<p>In a model 1 regression, you control one variable (x) and measure the response variable (y). Lab experiments are examples of this. In other situations, you do not control either variable, and often in these cases, it is not clear which variable would be treated as x and which would be treated as y. The order matters, because a regression of y on x produces a different line than a regression of x on y. An example in which you would not control either variable would be if you collected rocks and made two different measurements of those rocks. In this case, you do not control either value; you can only measure what is there. When you do not control one of the variables, both variables have measurement error and you must perform a model 2 regression. Model 2 regressions will allow us to describe the relationship and to test some hypotheses, but they cannot be used for prediction.</p>

<p>A model 2 regression accounts for the uncertainty in both x and y by minimizing the errors in both directions. There are several ways to do this. In a major axis regression, what is minimized is  the perpendicular distance from a point to the line. In standard major axis (SMA) regression (also called reduced major axis regression), the areas of the triangles formed by the observations and the regression line are minimized. The standard major axis regression is particularly common. The <b>slope</b> of a SMA regression is:</p>

<img src="./Model 2 regression_files/rmaSlope.png" alt="SMA slope">

<p>The sign is listed as plus or minus because it is set to match the sign of the correlation coefficient. The slope can be calculated as the ratio of the standard deviations or as the square-root of the ratio of the sum of squares, whichever is more convenient.</p>

<p>The SMA <b>y-intercept</b> is calculated as it is for the least-squares regression, that is, the line must pass through the centroid.</p>

<p>Functions for the SMA slope and intercept are straightforward.</p>

<p class="codeParagraph">
smaSlope &lt;- function(x,y) {<br>
&nbsp;&nbsp;&nbsp;b1 &lt;- sd(y)/sd(x)<br>
&nbsp;&nbsp;&nbsp;if (cor(x,y)&lt;0) b1 &lt;- -b1 # change sign for negative correlation<br>
&nbsp;&nbsp;&nbsp;b1<br>
}<br>
&nbsp;<br>
smaIntercept &lt;- function(x,y) {<br>
&nbsp;&nbsp;&nbsp;b1 &lt;- sd(y)/sd(x)<br>
&nbsp;&nbsp;&nbsp;if (cor(x,y)&lt;0) b1 &lt;- -b1 # change sign for negative correlation<br>
&nbsp;&nbsp;&nbsp;b0 &lt;- mean(y) - mean(x)*b1<br>
&nbsp;&nbsp;&nbsp;b0<br>
}
</p>

<p>The SMA slope equals the least-squares slope divided by the correlation coefficient and is therefore always steeper than a least-squares slope. The difference in these two slopes decreases as the correlation becomes stronger. As the correlation between two variables weakens, the slope of an SMA regression approaches 1.0, whereas it approaches 0 in a least-squares regression.</p>

<p><b>Standard errors</b> are available for the SMA slope and intercept, and from these, you can calculate confidence intervals on the slope and intercept. These confidence intervals are calculated with n-2 degrees of freedom.</p>

<img src="./Model 2 regression_files/rmaSEslope.png" alt="standard error of SMA slope">

<img src="./Model 2 regression_files/rmaSEintercept.png" alt="standard error of SMA intercept">

<p>The <span class="codeSpan">lmodel2</span> package can run a variety of model 2 regressions, plot them, and perform statistical tests. After loading that library, running <span class="codeSpan">vignette('mod2user')</span> will display an outstanding pdf on best practices, particularly the appropriate circumstances for each type of model 2 regression. If you think you might need a model 2 regression, read this pdf.</p>


<h2>Adding a trend line to a plot</h2>

<p>In some cases, you may wish to add a trend line through the data solely to describe any trends, which may not fit any prescribed function.</p>

<p>You can show how these could be used for a plot of temperature versus day of year, which displays a sinusoidal relationship. For this, use the UsingR library that accompanies the Crawley textbook.</p>

<p class="codeParagraph">
library(UsingR)<br>
attach(five.yr.temperature)
</p>


<p>The function <span class="codeSpan">scatter.smooth()</span> plots the data and the trend line in one step. The <span class="codeSpan">col</span> parameter sets the color of the data points.</p>

<p class="codeParagraph">
scatter.smooth(temps ~ days, col='gray')
</p>

<p>There are two ways you can add a trend line to an existing plot. The first uses the <span class="codeSpan">smooth.spline()</span> function to calculate the trend line and the <span class="codeSpan">lines()</span> function to add it to the plot. You can use the <span class="codeSpan">lty</span> parameter to dash the trend line and the <span class="codeSpan">lwd</span> parameter to bold it.</p>

<p class="codeParagraph">
plot(days, temps, col='gray')<br>
lines(smooth.spline(temps ~ days), lty=2, lwd=2)
</p>

<p>You can use the <span class="codeSpan">supsmu()</span> function to make a Friedman’s SuperSmoother trend line, and again display it with the <span class="codeSpan">lines()</span> function. Note that the syntax for calling <span class="codeSpan">supsmu()</span> differs from that of <span class="codeSpan">smooth.spline()</span> or performing a regression.</p>

<p class="codeParagraph">
plot(days, temps, col='gray')<br>
lines(supsmu(days, temps), lty=3, lwd=2)
</p>

<p>You can show all three trend lines on one plot for comparison. The default <span class="codeSpan">scatter.smooth()</span> function produces the smoothest trend line of the three. You can control the smoothness of any of these trend lines by adjusting one of their parameters. See the help pages for more details, in particular the <span class="codeSpan">span</span> parameter for <span class="codeSpan">scatter.smooth()</span>, the <span class="codeSpan">spar</span> parameter for <span class="codeSpan">smooth.spline()</span>, and the <span class="codeSpan">bass</span> parameter for <span class="codeSpan">supsmu()</span>.</p>

<p class="codeParagraph">
scatter.smooth(temps ~ days, col='gray')<br>
lines(supsmu(days, temps), lty=3, lwd=2)<br>
lines(smooth.spline(temps ~ days), lty=2, lwd=2)<br>
legend(locator(1), lty=c(1,2,3), lwd=c(1,2,2), legend=c('scatter.smooth', 'smooth.spline', 'supsmu'))
</p>

<p>The last line of this code adds a key with the <span class="codeSpan">legend()</span> function. In it, you specify the types of lines (<span class="codeSpan">lty</span>), their weight (<span class="codeSpan">lwd</span>), and their labels (<span class="codeSpan">legend</span>). The <span class="codeSpan">locator(1)</span> function lets you click where you want the upper left corner of the legend box to be placed on your plot. You could instead specify the (X,Y) coordinates of the upper left corner - see the help page for <span class="codeSpan">legend()</span> for instructions. The <span class="codeSpan">legend()</span> function can be used on any type of plot with different kinds of points.</p>



	
	</div>
</div>

<div id="Footer">
	<p>All original material Copyright 2015. No part of this site may be reproduced without written permission.</p>

	<p>The content and opinions expressed on this web page do not necessarily reflect the views of<br>nor are they endorsed by the University of Georgia or the University System of Georgia.</p>

</div>



</body></html>